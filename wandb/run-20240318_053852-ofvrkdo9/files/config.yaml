wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.10.13
    cli_version: 0.16.4
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1710740332.0
    t:
      1:
      - 1
      - 41
      - 50
      - 55
      - 77
      2:
      - 1
      - 41
      - 50
      - 55
      - 77
      3:
      - 13
      - 23
      4: 3.10.13
      5: 0.16.4
      8:
      - 5
      13: linux-x86_64
data:
  desc: null
  value:
    train:
      src: data/is2re/train/is2re_10k.lmdb
      normalize_labels: true
      target_mean: -1.525913953781128
      target_std: 2.279365062713623
    val:
      val_id:
        src: data/is2re/val/val_id.lmdb
    frame_averaging: ''
    fa_method: ''
name:
  desc: null
  value: test
model:
  desc: null
  value:
    name: faenet
    act: swish
    loss: mae
    hidden_channels: 384
    num_filters: 480
    num_interactions: 5
    num_gaussians: 104
    dropout_lin: 0.0
    dropout_edge: 0.0
    dropout_lowest_layer: output
    first_trainable_layer: ''
    cutoff: 6.0
    use_pbc: true
    regress_forces: false
    tag_hidden_channels: 64
    pg_hidden_channels: 64
    phys_embeds: true
    phys_hidden_channels: 0
    energy_head: weighted-av-final-embeds
    skip_co: concat
    second_layer_MLP: false
    complex_mp: true
    mp_type: base
    graph_norm: true
    force_decoder_type: mlp
    force_decoder_model_config:
      simple:
        hidden_channels: 128
        norm: batch1d
      mlp:
        hidden_channels: 256
        norm: batch1d
      res:
        hidden_channels: 128
        norm: batch1d
      res_updown:
        hidden_channels: 128
        norm: batch1d
optimizer:
  desc: null
  value:
    batch_size: 16
    eval_batch_size: 16
    epochs: 1
    scheduler: LinearWarmupCosineAnnealingLR
    optimizer: AdamW
    lr_initial: 0.0002
    lr_gamma: 0.1
    energy_grad_coefficient: 10
logger:
  desc: null
  value: wandb
project:
  desc: null
  value: grm-faenet
